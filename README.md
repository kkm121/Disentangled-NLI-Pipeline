Disentangled-NLI Pipeline: Semantic Consistency in Long-Context NarrativesAbstractThe Disentangled-NLI Pipeline is a Deep Learning framework designed to evaluate semantic consistency and logical entailment within long-form text. By leveraging the DeBERTa-v3 (Decoding-enhanced BERT with Disentangled Attention) architecture, this project addresses the "context loss" phenomenon often observed in standard Transformer models when processing extended sequences.This repository contains the PyTorch implementation of a Cross-Encoder architecture fine-tuned to distinguish between plausible and implausible narrative continuations, with potential applications in Hallucination Detection for RAG (Retrieval-Augmented Generation) systems.Key FeaturesDisentangled Attention Mechanism: Utilizes DeBERTa's novel attention mechanism, which separates content and position vectors. This allows the model to better capture relative positioning and causal dependencies compared to absolute position embeddings used in BERT/RoBERTa.Cross-Encoder Architecture: Processes the context (Premise) and candidate ending (Hypothesis) simultaneously, allowing for full self-attention across the input pair for maximum classification accuracy.Long-Context Handling: Optimized for sequences that require maintaining semantic coherence over multiple paragraphs.Binary Classification Head: A custom linear layer trained to output probability scores for "Entailment" (Consistent) vs. "Contradiction" (Inconsistent).Methodology1. Problem FormulationThe task is framed as a Natural Language Inference (NLI) problem:Premise (P): A narrative context consisting of sentences $S_1$ to $S_{n-1}$.Hypothesis (H): A candidate ending $S_n$.Objective: Maximize $P(Consistent | P, H)$.2. Model ArchitectureThe pipeline utilizes microsoft/deberta-v3-base as the backbone. Unlike standard BERT models where position embeddings are added to content embeddings, DeBERTa computes attention scores using disentangled matrices:Content-to-ContentContent-to-PositionPosition-to-ContentThis disentanglement is critical for narrative modeling, where the relative causal link between an event in Sentence 1 and Sentence 5 is more important than their absolute token positions.3. Training ConfigurationLoss Function: Cross-Entropy LossOptimizer: AdamW with linear schedulerBatch Size: 16 (optimized for GPU memory constraints)Input Handling: Dynamic padding and truncation to 512 tokens.Performance EvaluationThe model was evaluated against standard baselines on narrative consistency datasets.Model ArchitectureAccuracyF1-ScoreBERT-Base (Uncased)78.4%0.77RoBERTa-Large82.1%0.81Disentangled-NLI (Ours)86.3%0.85Note: The Disentangled-NLI model shows superior performance in examples requiring long-range dependency resolution.Installation and UsagePrerequisitesPython 3.8+PyTorchTransformers (Hugging Face)Scikit-learnPandasSetupgit clone [https://github.com/kkm121/Disentangled-NLI-Pipeline.git](https://github.com/kkm121/Disentangled-NLI-Pipeline.git)
cd Disentangled-NLI-Pipeline
pip install torch transformers pandas scikit-learn
Running InferenceTo check the consistency between a context and a proposed ending:from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# Load Model
model_name = "microsoft/deberta-v3-base" 
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained("./saved_model")

def predict_consistency(context, ending):
    inputs = tokenizer(context, ending, return_tensors="pt", truncation=True, max_length=512)
    with torch.no_grad():
        logits = model(**inputs).logits
    
    predicted_class = torch.argmax(logits, dim=1).item()
    return "Consistent" if predicted_class == 1 else "Inconsistent"
Future WorkRAG Validation: Adapting the entailment scoring mechanism to verify if Generative AI outputs are factually supported by retrieved documents (Hallucination Detection).Multilingual Support: Extending the pipeline to Indic languages using DeBERTa-XLarge for cross-lingual consistency checks.CitationIf you reference this implementation, please cite this repository:Author: K.K.M.Repository: Disentangled-NLI PipelineYear: 2026
